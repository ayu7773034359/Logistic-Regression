{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "\n",
        "   -Logistic Regression and Linear Regression are both supervised learning algorithms used in machine learning, but they are used for different types of problems and have different mathematical foundations.\n",
        "\n",
        "   Logistic Regression is a classification algorithm used when the target variable is categorical, usually binary (e.g., yes/no, 0/1, true/false). Despite the name, it's used for classification, not regression.\n",
        "\n",
        "  It predicts the probability that a given input point belongs to a certain class.\n",
        "\n",
        "  The output of logistic regression is passed through a sigmoid function (S-shaped curve) to map it between 0 and 1.\n",
        "\n",
        "\n",
        "  Linear Regression is used for predicting continuous values.\n",
        "\n",
        "  It models the relationship between the dependent variable and one or more independent variables by fitting a straight line (linear equation) to the data.\n",
        "\n",
        "  The output can be any real number.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lxm9AVgIE_HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "  -The goal of Logistic Regression is to model the probability that a data point belongs to a particular class (usually class 1), given its features.\n",
        "\n",
        "  \n",
        "  1. Linear Combination (like in Linear Regression):\n",
        "We first compute a weighted sum of the input features:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Or in vector form:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "𝑇\n",
        "𝑥\n",
        "z=β\n",
        "T\n",
        " x\n",
        "\n",
        "\n",
        "2. Apply the Sigmoid Function (Logistic Function):\n",
        "The sigmoid function maps any real value of\n",
        "𝑧\n",
        "z into a range between 0 and 1:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "So the logistic regression model becomes:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n"
      ],
      "metadata": {
        "id": "H7pqi-m7E_Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "\n",
        "  -We use the sigmoid function in logistic regression because it transforms the output of a linear equation (which can be any real number) into a probability value between 0 and 1, which is essential for binary classification.\n",
        "\n",
        "   1. Probability Interpretation\n",
        "The core idea of logistic regression is to predict the probability that an input belongs to class 1 (or class 0). Probabilities must lie between 0 and 1, but the output of a linear model like:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "can be any value from\n",
        "−\n",
        "∞\n",
        "−∞ to\n",
        "+\n",
        "∞\n",
        "+∞.\n",
        "\n",
        " The sigmoid function maps this to a bounded range:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "which gives values in\n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        " which gives values in (0,1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Szkr5UdDE_BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "\n",
        "  -The cost function in logistic regression is designed to measure how well the model's predicted probabilities match the actual class labels. Since we're dealing with probabilities, the Mean Squared Error (MSE) used in linear regression doesn't work well here.\n",
        "\n",
        "So, we use the Log Loss (also called Binary Cross-Entropy Loss) instead.\n",
        "\n",
        " Binary Cross-Entropy (Log Loss)\n",
        "For a single training example:\n",
        "\n",
        "Cost\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "−\n",
        "[\n",
        "𝑦\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "]\n",
        "Cost(h\n",
        "θ\n",
        "​\n",
        " (x),y)=−[y⋅log(h\n",
        "θ\n",
        "​\n",
        " (x))+(1−y)⋅log(1−h\n",
        "θ\n",
        "​\n",
        " (x))]\n",
        "Where:\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x) is the predicted probability (output of sigmoid),\n",
        "\n",
        "𝑦\n",
        "∈\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "y∈{0,1} is the true label.\n",
        "\n"
      ],
      "metadata": {
        "id": "p60C7jP6E--i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "  -Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function of logistic regression.\n",
        "\n",
        "It discourages the model from fitting too closely to the training data by shrinking the model’s coefficients (i.e., the weights\n",
        "𝛽\n",
        "β) — especially when they are too large.\n",
        "\n",
        "Why is Regularization Needed?\n",
        "Without regularization:\n",
        "\n",
        "The model might learn noise and outliers from training data.\n",
        "\n",
        "It becomes overly complex with large weights.\n",
        "\n",
        "It performs very well on training data, but poorly on unseen/test data (overfitting).\n",
        "\n",
        "Regularization controls model complexity and improves generalization to new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "syYQygmUE-7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "  -1. Ridge Regression (L2 Regularization)\n",
        "\n",
        " What it does:\n",
        "\n",
        "Adds a penalty on the sum of the squares of the coefficients.\n",
        "\n",
        "Shrinks large coefficients toward zero but never exactly zero.\n",
        "\n",
        "Keeps all features, just reduces their influence.\n",
        "\n",
        "2. Lasso Regression (L1 Regularization)\n",
        "\n",
        " What it does:\n",
        "\n",
        "Adds a penalty on the sum of the absolute values of the coefficients.\n",
        "\n",
        "Can shrink some coefficients exactly to zero, performing feature selection.\n",
        "\n",
        "3. Elastic Net Regression\n",
        "\n",
        " What it does:\n",
        "\n",
        "Combines both L1 (Lasso) and L2 (Ridge) penalties.\n",
        "\n",
        "Useful when there are many correlated features or when Lasso alone is too aggressive.\n",
        "\n"
      ],
      "metadata": {
        "id": "oV47OQgYE-45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "  -You should use Elastic Net instead of Lasso or Ridge when you want to combine their strengths and overcome their individual limitations, especially in situations like the ones below:\n",
        "\n",
        "  Use Elastic Net when:\n",
        "\n",
        "  1. You have many correlated features\n",
        " Lasso tends to pick only one feature from a group of correlated features and ignore the rest.\n",
        "\n",
        "- Ridge keeps all correlated features, but doesn't do feature selection.\n",
        "\n",
        "- Elastic Net balances this by selecting groups of correlated features together.\n",
        "\n",
        " 2. You have more features than observations (High-dimensional data)\n",
        " Example: 10,000 genes (features) and 200 patients (samples).\n",
        "\n",
        "- Lasso struggles here and may select too few features or behave unstably.\n",
        "\n",
        "- Elastic Net handles this better by stabilizing the selection with the Ridge component.\n",
        "\n",
        " 3. You suspect that only some features are important, but not too few\n",
        " If only a few features matter → use Lasso\n",
        "\n",
        "- If all features matter → use Ridge\n",
        "\n",
        "- If you're unsure and want a balance → use Elastic Net\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rdxJ-VR6E-2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "\n",
        "- The regularization parameter\n",
        "𝜆\n",
        "λ (lambda) controls the strength of the penalty applied to the model’s weights in regularized logistic regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "zKrx7PlGE-zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "\n",
        " -Here are the key assumptions of Logistic Regression:\n",
        "\n",
        " 1. Binary or Multiclass Dependent Variable\n",
        "\n",
        "- The dependent variable (target) should be binary (0 or 1) for binary logistic regression.\n",
        "\n",
        "- For multiclass logistic regression (multinomial), the target variable should be categorical with more than two classes.\n",
        "\n",
        "2. Independence of Observations\n",
        "\n",
        "- Each observation in the dataset should be independent.\n",
        "\n",
        "- There should be no duplicates or repeated measures for the same subject unless you're using special models like mixed effects.\n",
        "\n",
        "3. No Perfect Multicollinearity\n",
        "\n",
        "- The independent variables (features) should not be highly correlated with each other.\n",
        "\n",
        "- High correlation between predictors (multicollinearity) can make coefficient estimates unreliable.\n",
        "\n",
        "4. Linearity of Logit\n",
        "\n",
        "- Logistic regression assumes a linear relationship between the independent variables and the log odds of the dependent variable (not between predictors and the target itself).\n",
        "\n",
        "This means:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "where\n",
        "𝑝\n",
        "p is the probability of the positive class.\n",
        "\n",
        "5. Large Sample Size\n",
        "\n",
        "- Logistic regression requires a sufficiently large sample size for stable and reliable estimates.\n",
        "\n",
        "- Rule of thumb: at least 10 events (positive class cases) per predictor.\n",
        "\n",
        "6. Little to No Outliers\n",
        "\n",
        "- Logistic regression is sensitive to outliers in the independent variables.\n",
        "\n",
        "- Outliers can disproportionately affect model performance."
      ],
      "metadata": {
        "id": "_SdxHMWFE-wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "  - Here are some popular alternatives to Logistic Regression for classification tasks, especially when its assumptions are not met or better performance is needed:\n",
        "\n",
        "1. Decision Trees\n",
        "\n",
        "- Splits data into branches to make decisions.\n",
        "\n",
        "- No assumptions about data distribution.\n",
        "\n",
        "- Pros: Easy to interpret, handles non-linear data.\n",
        "\n",
        "- Cons: Prone to overfitting.\n",
        "\n",
        "2. Random Forest\n",
        "\n",
        "- An ensemble of decision trees using bagging.\n",
        "\n",
        "- Pros: Reduces overfitting, handles missing values well.\n",
        "\n",
        "- Cons: Less interpretable, slower for large datasets.\n",
        "\n",
        "3. Gradient Boosting Machines (GBM, XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "- Build trees sequentially to correct previous errors.\n",
        "\n",
        "- Pros: Very high performance, handles mixed data types.\n",
        "\n",
        "- Cons: More complex to tune and slower to train.\n",
        "\n",
        "4. Support Vector Machines (SVM)\n",
        "\n",
        "- Finds the hyperplane that best separates classes.\n",
        "\n",
        "- Pros: Effective in high-dimensional spaces, can use kernel tricks for non-linear classification.\n",
        "\n",
        "- Cons: Hard to interpret, not suitable for very large datasets.\n",
        "\n",
        "5. K-Nearest Neighbors (KNN)\n",
        "\n",
        "- Classifies based on the majority class among k nearest data points.\n",
        "\n",
        "- Pros: Simple, no training needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "BubBj_e5E-tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "\n",
        "  -Classification evaluation metrics help you measure how well your model is performing in predicting categorical (class) labels. Here's a breakdown of the most commonly used metrics:\n",
        "\n",
        "\n",
        "1. Accuracy\n",
        "Definition: Percentage of correct predictions.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "​\n",
        "\n",
        "Good for: Balanced datasets.\n",
        "\n",
        "Not reliable for imbalanced datasets (e.g., 95% of one class).\n",
        "\n",
        "2. Precision\n",
        "Definition: Of all positive predictions, how many were correct?\n",
        "\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        "\n",
        "Use when: False positives are costly (e.g., spam detection).\n",
        "\n",
        "3. Recall (Sensitivity / True Positive Rate)\n",
        "Definition: Of all actual positives, how many were correctly predicted?\n",
        "\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        "\n",
        "Use when: False negatives are costly (e.g., cancer detection).\n",
        "\n"
      ],
      "metadata": {
        "id": "S-P4axC4E-qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression.\n",
        "\n",
        "  -Class imbalance can negatively affect Logistic Regression and most classification models. Here’s how and why:\n",
        "\n",
        "  What is Class Imbalance?\n",
        "\n",
        " When one class (e.g., “0”) is much more frequent than the other class (e.g., “1”).\n",
        "\n",
        " Example: 95% negative cases, 5% positive cases (like fraud detection, disease diagnosis).\n",
        "\n",
        " How It Affects Logistic Regression:\n",
        "\n",
        " 1. Biased Predictions Toward the Majority Class\n",
        "\n",
        "Logistic regression minimizes the overall error, so it may favor the majority class to get high accuracy.\n",
        "\n",
        "Example: Model predicts “0” for everything and still gets 95% accuracy — but it's useless!\n",
        "\n",
        "2. Poor Recall for Minority Class\n",
        "\n",
        "The model may fail to detect rare events (false negatives increase).\n",
        "\n",
        "Critical in use cases like cancer or fraud detection.\n",
        "\n",
        "3. Misleading Accuracy\n",
        "\n",
        "High accuracy might hide poor model performance.\n",
        "\n",
        "Accuracy is not a good metric in imbalanced settings — use F1 Score, Precision, Recall, or AUC instead.\n",
        "\n",
        "4. Poor Probability Calibration\n",
        "\n",
        "Predicted probabilities may be less reliable, especially for the minority class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VQ__YyayE-n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "  -Hyperparameter tuning in Logistic Regression is the process of choosing the best set of parameters that are not learned from the data directly, but influence how the model learns. These hyperparameters affect the model’s performance, regularization, and convergence speed.\n",
        "\n",
        " Key Hyperparameters in Logistic Regression\n",
        "\n",
        " 1. Penalty (penalty)\n",
        "\n",
        "What it does: Adds regularization to prevent overfitting.\n",
        "\n",
        "Options:\n",
        "\n",
        "'l1': Lasso (feature selection, may shrink some coefficients to zero).\n",
        "\n",
        "'l2': Ridge (shrinks coefficients, keeps all features).\n",
        "\n",
        "'elasticnet': Combines L1 and L2 (use with l1_ratio).\n",
        "\n",
        "'none': No regularization (not recommended unless data is simple and clean).\n",
        "\n",
        "2. Regularization Strength (C)\n",
        "\n",
        "What it does: Controls how much regularization is applied.\n",
        "\n",
        "Note: Smaller C → stronger regularization.\n",
        "\n",
        "Range: Usually tested between 0.0001 to 1000 (log scale).\n",
        "\n",
        "3. Solver (solver)\n",
        "\n",
        "What it does: Algorithm used to fit the model.\n",
        "\n",
        "Common options:\n",
        "\n",
        "'liblinear': Good for small datasets and supports L1.\n",
        "\n",
        "'saga': Supports elastic net; works on large datasets.\n",
        "\n",
        "'lbfgs' and 'newton-cg': Faster for multiclass and L2 penalty.\n",
        "\n",
        "4. Maximum Iterations (max_iter)\n",
        "\n",
        "What it does: Maximum number of iterations allowed during optimization.\n",
        "\n",
        "Use when: The model doesn't converge.\n",
        "\n",
        "5. Elastic Net Mixing Ratio (l1_ratio)\n",
        "\n",
        "Only used with: penalty='elasticnet' and solver='saga'\n",
        "\n",
        "What it does: Controls the mix of L1 and L2:\n",
        "\n",
        "l1_ratio = 0: pure L2\n",
        "\n",
        "l1_ratio = 1: pure L1\n",
        "\n"
      ],
      "metadata": {
        "id": "cvlG_SZNE-lV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        "  -In Logistic Regression, the solver determines the optimization algorithm used to find the best parameters (weights). Different solvers work better under different conditions (data size, regularization type, sparsity, etc.).\n",
        "\n",
        "  How Each Solver Works (Brief Overview)\n",
        "\n",
        "1. liblinear\n",
        "\n",
        "Coordinate descent algorithm.\n",
        "\n",
        "Works well for small datasets.\n",
        "\n",
        "Supports L1 and L2 regularization.\n",
        "\n",
        "Does not support multiclass natively (ovr used).\n",
        "\n",
        "2. lbfgs (Limited-memory BFGS)\n",
        "\n",
        "Quasi-Newton method.\n",
        "\n",
        "Fast for large datasets.\n",
        "\n",
        "Supports only L2.\n",
        "\n",
        "Best for multiclass problems (with multi_class='multinomial').\n",
        "\n",
        "3. newton-cg\n",
        "\n",
        "Uses the Newton-Raphson method with conjugate gradient.\n",
        "\n",
        "Suitable for large datasets and multinomial classification.\n",
        "\n",
        "Only supports L2.\n",
        "\n",
        "4. sag (Stochastic Average Gradient)\n",
        "\n",
        "Faster on large datasets, especially when features > samples.\n",
        "\n",
        "Only supports L2 regularization.\n",
        "\n",
        "Needs feature scaling.\n",
        "\n",
        "5. saga\n",
        "\n",
        "Variant of sag; handles L1, L2, and Elastic Net.\n",
        "\n",
        "Good for sparse data (like text) and large-scale problems.\n",
        "\n",
        "Can be used with all types of penalties.\n",
        "\n"
      ],
      "metadata": {
        "id": "cyZ4F-P-E-i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        " -Logistic Regression can be extended to multiclass classification using specialized techniques, since standard logistic regression is inherently binary (2-class). Here’s how it works:\n",
        "\n",
        " 1. One-vs-Rest (OvR) / One-vs-All\n",
        "\n",
        "  How it works:\n",
        "\n",
        " Trains one binary classifier per class.\n",
        "\n",
        " For each class\n",
        " 𝑖\n",
        " i, it predicts:\n",
        "\n",
        " Is this sample class\n",
        " 𝑖\n",
        " i vs. all other classes?\n",
        "\n",
        " 2. Multinomial (Softmax) Logistic Regression\n",
        "\n",
        " How it works:\n",
        "\n",
        " Uses a single model that estimates the probability of each class directly.\n",
        "\n",
        " Applies the softmax function instead of sigmoid.\n",
        "\n",
        " 𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑖\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "𝑇\n",
        "𝑥\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝛽\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        "for\n",
        "𝑖\n",
        "=\n",
        "1\n",
        " to\n",
        "𝐾\n",
        "P(y=i∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "β\n",
        "j\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "e\n",
        "β\n",
        "i\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "​\n",
        " for i=1 to K\n",
        "\n"
      ],
      "metadata": {
        "id": "Umfy5GEEE-go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "  \n",
        "  -Here are the advantages and disadvantages of Logistic Regression explained clearly:\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "\n",
        "1. Simple and Easy to Implement\n",
        "\n",
        "Straightforward to understand and use.\n",
        "\n",
        "Good for baseline models and quick insights.\n",
        "\n",
        "2. Interpretable Results\n",
        "\n",
        "Coefficients show the direction and strength of relationship between features and the outcome.\n",
        "\n",
        "3. Works Well with Linearly Separable Data\n",
        "\n",
        "Performs well when classes can be separated by a straight line or plane.\n",
        "\n",
        "4. Efficient and Fast\n",
        "\n",
        "Requires less computational power than more complex models.\n",
        "\n",
        "Good for small to medium-sized datasets.\n",
        "\n",
        "5. Probabilistic Output\n",
        "\n",
        "Outputs probabilities (using the sigmoid function), which are useful for threshold tuning and ranking.\n",
        "\n",
        "6. Regularization Support\n",
        "\n",
        "Can use L1, L2, or Elastic Net regularization to prevent overfitting."
      ],
      "metadata": {
        "id": "geVWde-JE-dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical\n"
      ],
      "metadata": {
        "id": "-0ksYQN7MEeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "l7YSDd5_E-bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data     # Features\n",
        "y = iris.target   # Target\n",
        "\n",
        "# Step 2: Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)  # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-ePpw1cMH6J",
        "outputId": "32596627-6170-453c-8c1c-fa2a7a6c4408"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy"
      ],
      "metadata": {
        "id": "b6h-3YFjE-Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling (important for L1 regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Create and train the logistic regression model with L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='saga', max_iter=500, multi_class='ovr')  # solver='saga' supports L1\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L1 Regularization Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97WdHnE7MaKR",
        "outputId": "84aa79a0-46a6-4989-8216-4a5bc3f2e9ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L1 Regularization Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients"
      ],
      "metadata": {
        "id": "KLpMyhuiE-WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale features (recommended for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=500, multi_class='ovr')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression with L2 Regularization Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 7: Print coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "# The coefficients are stored in model.coef_ and the intercept in model.intercept_\n",
        "# For multiclass (ovr), coef_ has shape (n_classes, n_features) and intercept_ has shape (n_classes,)\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"  Class {class_name} (vs Rest):\")\n",
        "    coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_[i]})\n",
        "    print(coef_df)\n",
        "    print(f\"  Intercept: {model.intercept_[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SroHV0bMlMS",
        "outputId": "ed4b16ae-2b0e-4e83-9993-1296a160ae0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 Regularization Accuracy: 0.97\n",
            "\n",
            "Model Coefficients:\n",
            "  Class setosa (vs Rest):\n",
            "             Feature  Coefficient\n",
            "0  sepal length (cm)    -0.999106\n",
            "1   sepal width (cm)     1.183028\n",
            "2  petal length (cm)    -1.672349\n",
            "3   petal width (cm)    -1.539707\n",
            "  Intercept: -2.3204\n",
            "  Class versicolor (vs Rest):\n",
            "             Feature  Coefficient\n",
            "0  sepal length (cm)     0.253263\n",
            "1   sepal width (cm)    -1.284582\n",
            "2  petal length (cm)     0.557621\n",
            "3   petal width (cm)    -0.741569\n",
            "  Intercept: -0.8913\n",
            "  Class virginica (vs Rest):\n",
            "             Feature  Coefficient\n",
            "0  sepal length (cm)     0.176849\n",
            "1   sepal width (cm)    -0.658311\n",
            "2  petal length (cm)     2.319517\n",
            "3   petal width (cm)     2.857351\n",
            "  Intercept: -3.7130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "g_RBY2rkE-Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale features (important for regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',        # saga supports elasticnet\n",
        "    l1_ratio=0.5,         # 0.5 means equal mix of L1 and L2\n",
        "    max_iter=500,\n",
        "    multi_class='ovr'\n",
        ")\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy and coefficients\n",
        "print(f\"Logistic Regression with Elastic Net Regularization Accuracy: {accuracy:.2f}\\n\")\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Model Coefficients (per class):\")\n",
        "coef_df = pd.DataFrame(model.coef_, columns=feature_names, index=class_names)\n",
        "print(coef_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvUsqGEiM-L7",
        "outputId": "624c869d-c5ee-41dd-a718-15cb2334a827"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Elastic Net Regularization Accuracy: 0.97\n",
            "\n",
            "Model Coefficients (per class):\n",
            "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "setosa              -0.799657          1.127548          -1.906842   \n",
            "versicolor           0.246069         -1.302723           0.421680   \n",
            "virginica            0.000000         -0.681016           2.744508   \n",
            "\n",
            "            petal width (cm)  \n",
            "setosa             -1.662479  \n",
            "versicolor         -0.604980  \n",
            "virginica           3.290677  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'"
      ],
      "metadata": {
        "id": "Ksl6_3M7E-RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Iris dataset (multiclass)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with multi_class='ovr'\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=500)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multiclass Logistic Regression (OvR) Accuracy: {accuracy:.2f}\\n\")\n",
        "\n",
        "# Step 7: Print coefficients\n",
        "print(\"Model Coefficients (per class):\")\n",
        "coef_df = pd.DataFrame(model.coef_, columns=feature_names, index=class_names)\n",
        "print(coef_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDSrGBj0QuYs",
        "outputId": "21bd9e51-ff19-4f43-fcb5-bd5eec243bb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression (OvR) Accuracy: 0.97\n",
            "\n",
            "Model Coefficients (per class):\n",
            "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "setosa              -0.779293          1.351991          -1.596273   \n",
            "versicolor           0.251131         -1.266962           0.550784   \n",
            "virginica            0.018031         -0.208279           1.735295   \n",
            "\n",
            "            petal width (cm)  \n",
            "setosa             -1.427373  \n",
            "versicolor         -0.739319  \n",
            "virginica           2.392299  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "MiVtj-XkE-OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Define model and parameter grid\n",
        "model = LogisticRegression(solver='saga', max_iter=1000, multi_class='ovr')  # saga supports both l1 and l2\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Step 5: Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Print best parameters and evaluate on test set\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM7okfsWQ4ry",
        "outputId": "dc3214aa-3e46-46d4-f24b-45f9fe9985c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Test Accuracy with Best Model: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aMWh7z3XE-L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ck1RFSOLE-Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4YNd4hoHE-G3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uL4ZOLgTE-Ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v-XK0bJDE-Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3dy7cj_DE9_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy0V5SwPE38p"
      },
      "outputs": [],
      "source": []
    }
  ]
}